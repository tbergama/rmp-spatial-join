{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the file produced by `assign_polys.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data file\n",
    "# Note that you may need to add an extra slash (\"\\\") in front of the existing slashes to avoid errors\n",
    "data_path = Path(\"C:\\\\Users\\\\kodonnel\\\\Documents\\\\Projects\\\\ToePulse\\\\TP_testSpatialJoin.csv\")\n",
    "\n",
    "df_data = pd.read_csv(data_path)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A little bit of clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense to compute statistics for some columns in the dataset (like latitude and longitude), so lets identify the columns we don't want to include and drop them from the dataframe.\n",
    "\n",
    "*Note: we could do this the other way - by identifying the columns we want to keep - but it is my hope that the columns we want to drop will be more or less consistant from dataset to dataset, more so than the constituent columns. So hopefully identifying the columns to drop will mean that little or no modification is needed from dataset to dataset. We will see...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets see what columns we have\n",
    "df_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make a list of the ones we don't want to include\n",
    "# I am just copy/pasting non-constituent columns from above into this list\n",
    "cols_to_drop = [\n",
    "    'FTS Timestamp', \n",
    "    'FTS Latitude', \n",
    "    'FTS Longitude',\n",
    "    'UCI Timestamp',\n",
    "     'index_right',\n",
    "    'Shape_Leng', 'Shape_Area'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.drop(cols_to_drop, axis=\"columns\")\n",
    "df_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool now we just have columns that we are going to use in our statistics computation. Now we need to identify the column that contains the polygon id that was assigned by `assign_polys.py`.\n",
    "\n",
    "In this case, the column is **`'CL_ID'`**\n",
    "\n",
    "But keep in mind that this could be different in a different dataset. Look for something that looks like it means \"centerline id\" or \"polygon id\".\n",
    "\n",
    "We can't do anything with rows that do not have a polygon id, so the next step will be to drop any rows with a missing polygon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a variable for the polygon id column name\n",
    "# Change this to the name you identified in the list above\n",
    "poly_id_col = \"Poly_ID2\"\n",
    "\n",
    "print(len(df_data))\n",
    "df_data = df_data.dropna(subset=[poly_id_col])\n",
    "print(len(df_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length updated if there were any rows missing a polygon id. Now we are ready to compute statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute statistics: Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First group the data by poly_id_col\n",
    "df_grouped = df_data.groupby(poly_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define the statistics we want to compute in a list that we can pass to the pandas aggregation function\n",
    "# For more information on what can go into this list check out: \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html\n",
    "#KO: added std to stats list 5/19/2020\n",
    "\n",
    "stats = [\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"mean\",\n",
    "    \"median\",\n",
    "    \"std\"\n",
    "]\n",
    "\n",
    "# Compute the statistics defined above for each polygon\n",
    "df_stats = df_grouped.agg(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the hierarchical columns\n",
    "df_stats.columns = [' '.join(col).strip() for col in df_stats.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Outlier removal and re-run stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged df_data (with original, input data) with dt_stats on Poly ID. \n",
    "<br>Might need to change Poly_ID2 to CL_ID, depending on how the spatial join code runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all field values that are greater than 2 times the standard deviation (2xstd)\n",
    "\n",
    "If the difference between the field value and its corresponding polygon mean value is greater \n",
    "than 2xstd of that polygon.\n",
    "\n",
    "All field values considered outliers are replaced with NaN. \n",
    "\n",
    "This process is iterative - user determines how many times to run the loop, and thus how many times to re-filter data for outlier removal. Mean and standard deviation are re-computed with each loop and the 'new' values are used to determine outliers. \n",
    "\n",
    "Summation of all standard deviations is printed to track convergence and can be used as a guide for the number of times to loop.\n",
    "\n",
    "Each loop writes overwrite the data in the file from the previous loop - data resulting from final loop is what will be in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the loop desired number of times. final output file will have the stats based on the final loop. \n",
    "# adjust number of iterations to make sure values converge using printed stat below\n",
    "num_iterations = 12\n",
    "tmp_df_data = df_data.copy() #copy of original df_data to preserve data\n",
    "tmp_df_stats = df_stats.copy() \n",
    "for num in range(num_iterations):\n",
    "    \n",
    "    # merge original data and stats dataframes on poly ID\n",
    "    df_merged = df_data.merge(tmp_df_stats, how = 'outer', left_on = 'Poly_ID2', right_on = 'Poly_ID2', )\n",
    "    \n",
    "    convergence = 0\n",
    "    for col in df_data.columns:\n",
    "        # skip location, poly_id, lat, long columns when filtering data\n",
    "        if col in ('Location','Poly_ID2','Latitude','Longitude'):\n",
    "            continue \n",
    "\n",
    "        # creates series to hold mean and standard deviation values\n",
    "        col_mean = col + ' mean' \n",
    "        col_std = col + ' std' # standard deviation\n",
    "\n",
    "        # calculates and creates a column for 2x std\n",
    "        col_2xstd = 2*df_merged[col_std]\n",
    "\n",
    "        # calculates the difference between the instantaneous value and the mean value of its corresponding polygon\n",
    "        col_diff_mean = abs(df_merged[col] - df_merged[col_mean])\n",
    "\n",
    "        # if the difference between the instantenous value and the mean is greater than 2xstd replace the value with a NAN\n",
    "        remove_mask = col_diff_mean > col_2xstd\n",
    "        df_merged[col][remove_mask] = np.nan   \n",
    "        convergence += col_2xstd.sum()\n",
    "    print(\"Sum of standard deviations converging on: \", convergence)\n",
    "\n",
    "    # creates dataframe, df_data_filtered\n",
    "    # dropping extra columns created during filtering\n",
    "    # filtered data retained, and columns kept are based on columns that existed in df_data\n",
    "    tmp_df_data = df_merged[df_data.columns]\n",
    "    \n",
    "    # Re-run stats on filtered data\n",
    "    df_grouped_filtered = tmp_df_data.groupby(poly_id_col)\n",
    "\n",
    "    # Compute the statistics defined above for each polygon\n",
    "    tmp_df_stats = df_grouped_filtered.agg(stats)\n",
    "\n",
    "    # Flatten the hierarchical columns\n",
    "    tmp_df_stats.columns = [' '.join(col).strip() for col in tmp_df_stats.columns.values]\n",
    "\n",
    "# change name of dataframe for clarity after processing\n",
    "df_stats_filtered = tmp_df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for each polygon based on final iteration of above loop\n",
    "\n",
    "df_stats_filtered.to_excel('data_stats_filtered.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
