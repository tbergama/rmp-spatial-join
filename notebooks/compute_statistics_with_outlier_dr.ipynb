{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the file produced by `assign_polys.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data file\n",
    "# Note that you may need to add an extra slash (\"\\\") in front of the existing slashes to avoid errors\n",
    "data_path = Path(\"C:\\\\Users\\\\kodonnel\\\\Documents\\\\Projects\\\\ToePulse\\\\TP_testSpatialJoin.csv\")\n",
    "\n",
    "df_data = pd.read_csv(data_path)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A little bit of clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense to compute statistics for some columns in the dataset (like latitude and longitude), so lets identify the columns we don't want to include and drop them from the dataframe.\n",
    "\n",
    "*Note: we could do this the other way - by identifying the columns we want to keep - but it is my hope that the columns we want to drop will be more or less consistant from dataset to dataset, more so than the constituent columns. So hopefully identifying the columns to drop will mean that little or no modification is needed from dataset to dataset. We will see...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets see what columns we have\n",
    "df_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make a list of the ones we don't want to include\n",
    "# I am just copy/pasting non-constituent columns from above into this list\n",
    "cols_to_drop = [\n",
    "    'FTS Timestamp', \n",
    "    'FTS Latitude', \n",
    "    'FTS Longitude',\n",
    "    'UCI Timestamp',\n",
    "     'index_right',\n",
    "    'Shape_Leng', 'Shape_Area'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.drop(cols_to_drop, axis=\"columns\")\n",
    "df_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool now we just have columns that we are going to use in our statistics computation. Now we need to identify the column that contains the polygon id that was assigned by `assign_polys.py`.\n",
    "\n",
    "In this case, the column is **`'CL_ID'`**\n",
    "\n",
    "But keep in mind that this could be different in a different dataset. Look for something that looks like it means \"centerline id\" or \"polygon id\".\n",
    "\n",
    "We can't do anything with rows that do not have a polygon id, so the next step will be to drop any rows with a missing polygon id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a variable for the polygon id column name\n",
    "# Change this to the name you identified in the list above\n",
    "poly_id_col = \"Poly_ID2\"\n",
    "\n",
    "print(len(df_data))\n",
    "df_data = df_data.dropna(subset=[poly_id_col])\n",
    "print(len(df_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length updated if there were any rows missing a polygon id. Now we are ready to compute statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute statistics: Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First group the data by poly_id_col\n",
    "df_grouped = df_data.groupby(poly_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define the statistics we want to compute in a list that we can pass to the pandas aggregation function\n",
    "# For more information on what can go into this list check out: \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html\n",
    "#KO: added std to stats list 5/19/2020\n",
    "\n",
    "stats = [\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"mean\",\n",
    "    \"median\",\n",
    "    \"std\"\n",
    "]\n",
    "\n",
    "# Compute the statistics defined above for each polygon\n",
    "df_stats = df_grouped.agg(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the hierarchical columns\n",
    "df_stats.columns = [' '.join(col).strip() for col in df_stats.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged df_data (with original, input data) with dt_stats on Poly ID. \n",
    "<br>Might need to change Poly_ID2 to CL_ID, depending on how the spatial join code runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge original data and stats dataframes on poly ID\n",
    "df_merged = df_data.merge(df_stats, how = 'outer', left_on = 'Poly_ID2', right_on = 'Poly_ID2', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all field values that are greater than 2 times the standard deviation (2xstd)\n",
    "\n",
    "If the difference between the field value and its corresponding polygon mean value is greater \n",
    "than 2xstd of that polygon.\n",
    "\n",
    "All field values considered outliers are replaced with NaN. \n",
    "\n",
    "This process is only done once, but could be amended to run iteratively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df_data.columns:\n",
    "    # skip location, poly_id, lat, long columns when filtering data\n",
    "    if col in ('Location','Poly_ID2','Latitude','Longitude'):\n",
    "        continue \n",
    "        \n",
    "    # creates series to hold mean, standard deviation, and 2xstd values\n",
    "    col_mean = col + ' mean' \n",
    "    col_std = col + ' std' # standard deviation\n",
    "      \n",
    "    # calculates and creates a column for 2x std\n",
    "    col_2xstd = 2*df_merged[col_std]\n",
    "    \n",
    "    # calculates the difference between the instantaneous value and the mean value of its corresponding polygon\n",
    "    col_diff_mean = abs(df_merged[col] - df_merged[col_mean])\n",
    "    \n",
    "    # if the difference between the instantenous value and the mean is greater than 2xstd replace the value with a NAN\n",
    "    remove_mask = col_diff_mean > col_2xstd\n",
    "    df_merged[col][remove_mask] = np.nan   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Re-run stats of filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe, df_data_filtered\n",
    "# dropping extra columns created during filtering\n",
    "# filtered data retained, and columns kept are based on columns that existed in df_data\n",
    "df_data_filtered = df_merged[df_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run stats on filtered data\n",
    "df_grouped_filtered = df_data_filtered.groupby(poly_id_col)\n",
    "\n",
    "# Compute the statistics defined above for each polygon\n",
    "df_stats_filtered = df_grouped_filtered.agg(stats)\n",
    "\n",
    "# Flatten the hierarchical columns\n",
    "df_stats_filtered.columns = [' '.join(col).strip() for col in df_stats_filtered.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_filtered.to_excel('data_stats_filtered_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
